{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# разметка данных\n",
    "\n",
    "train_texts = [\"«футболка»\",\n",
    "    \"«трусы»\",\n",
    "    \"«носки»\",\n",
    "    \"«джинсы»\",\n",
    "    \"«куртка»\",\n",
    "    \"«шорты»\",\n",
    "    \"«кроссовки»\",\n",
    "    \"«сандалии»\",\n",
    "    \"«зубная щётка»\",\n",
    "    \"«шампунь»\",\n",
    "    \"«мыло»\",\n",
    "    \"«расчёска»\",\n",
    "    \"«ноутбук»\",\n",
    "    \"«зарядка»\",\n",
    "    \"«наушники»\",\n",
    "    \"«паспорт»\",\n",
    "    \"«зонт»\",\n",
    "    \"«книга»\",\n",
    "    \"«вода»\",\n",
    "    \"«кинь две футболки»\",\n",
    "    \"«ноутбук и зарядка»\",\n",
    "    \"«зубная щётка»\",\n",
    "    \"«возьми трусы\",\n",
    "    \"носки»\",\n",
    "    \"«джинсы\",\n",
    "    \"куртка»\",\n",
    "    \"«не забудь зонт»\",\n",
    "    \"«два паспорта»\",\n",
    "    \"«сандалии и шорты»\",\n",
    "    \"«добавь шампунь»\",\n",
    "    \"«забери расчёску»\",\n",
    "    \"«кинь пару трусов»\",\n",
    "    \"«возьму куртку и шапку»\",\n",
    "    \"«не забудь про воду»\",\n",
    "    \"«захвати зонт и книгу»\",\n",
    "    \"«бери больше футболок»\",\n",
    "    \"«хватит одной зарядки»\",\n",
    "    \"«положу расчёску и мыло»\",\n",
    "    \"«нужно взять два паспорта»\",\n",
    "    \"«кинь футболку»\",\n",
    "    \"«трусы и носки»\",\n",
    "    \"«джинсы и куртка»\",\n",
    "    \"«забери зарядку»\",\n",
    "    \"«шампунь и мыло»\",\n",
    "    \"«расчёска и зубная щётка»\",\n",
    "    \"«два паспорта»\",\n",
    "    \"«сандалии и шорты»\",\n",
    "    \"«вода и еда»\",\n",
    "    \"«наушники и книга»\",\n",
    "    \"«не забудь зонт»\",\n",
    "    \"«футболка и шорты»\",\n",
    "    \"«тапки и халат»\",\n",
    "    \"«куртка и шапка»\",\n",
    "    \"«три футболки»\",\n",
    "    \"«ноутбук и мышь»\",\n",
    "    \"«расчёска и заколки»\",\n",
    "    \"«паспорт и билеты»\",\n",
    "    \"«салфетки и спрей»\",\n",
    "    \"«солнцезащитные очки»\",\n",
    "    \"«плеер и наушники»\",\n",
    "    \"«термокружка и ложка»\",\n",
    "    \"«зонт и перчатки»\",\n",
    "    \"«рюкзак и термос»\",\n",
    "    \"«купальник и полотенце»\",\n",
    "    \"«пижама и тапочки»\",\n",
    "    \"«фотоаппарат и карта памяти»\",\n",
    "    \"«аптечка и бинты»\",\n",
    "    \"«документы и деньги»\",\n",
    "    \"«кофта и свитер»\",\n",
    "    \"«ключи и документы»\",\n",
    "    \"«плеер и зарядка»\",\n",
    "    \"«еда и вода»\",\n",
    "    \"«книги и блокнот»\",\n",
    "    \"«плеер, наушники, зарядка»\",\n",
    "    \"«зубная щётка, паста»\",\n",
    "    \"«мыло, шампунь, гель»\",\n",
    "    \"«расчёска, заколки, резинки»\",\n",
    "    \"«одежда, обувь, аксессуары».\",\n",
    "    \"«кинь футболку»\",\n",
    "    \"«трусы и носки»\",\n",
    "    \"«джинсы и куртка»\",\n",
    "    \"«зарядка для ноутбука»\",\n",
    "    \"«шампунь и мыло»\",\n",
    "    \"«расчёска и зеркало»\",\n",
    "    \"«тапки и халат»\",\n",
    "    \"«паспорт и билеты»\",\n",
    "    \"«зонт и перчатки»\",\n",
    "    \"«вода и еда»\",\n",
    "    \"«наушники и книга»\",\n",
    "    \"«три футболки»\",\n",
    "    \"«две пары носков»\",\n",
    "    \"«одни джинсы»\",\n",
    "    \"«одна куртка»\",\n",
    "    \"«забери зарядку»\",\n",
    "    \"«не забудь расчёску»\",\n",
    "    \"«добавь тапочки»\",\n",
    "    \"«возьми шампунь»\",\n",
    "    \"«кинь зубную щётку»\",\n",
    "    \"«два паспорта»\",\n",
    "    \"«сандалии и шорты»\",\n",
    "    \"«солнцезащитные очки»\",\n",
    "    \"«фотоаппарат и карта памяти»\",\n",
    "    \"«плеер и наушники»\",\n",
    "    \"«ключи от дома»\",\n",
    "    \"«дождевик и зонт»\",\n",
    "    \"«спортивная одежда»\",\n",
    "    \"«подушка и одеяло»\",\n",
    "    \"«аптечка и пластырь»\",\n",
    "    \"«документы и деньги»\",\n",
    "    \"«рюкзак и сумка»\",\n",
    "    \"«тёплая одежда»\",\n",
    "    \"«лёгкая одежда»\",\n",
    "    \"«обувь для тренировок»\",\n",
    "    \"«кофта и шапка»\",\n",
    "    \"«ботинки и носки»\",\n",
    "    \"«косметичка и зеркало»\",\n",
    "    \"«еда и напитки»\",\n",
    "    \"«спальные принадлежности»\",\n",
    "    \"«купальник и полотенце»\",\n",
    "    \"«инструменты и принадлежности».\",\n",
    "    \"«футболка»\",\n",
    "    \"«трусы»\",\n",
    "    \"«носки»\",\n",
    "    \"«джинсы»\",\n",
    "    \"«куртка»\",\n",
    "    \"«тапки»\",\n",
    "    \"«кроссовки»\",\n",
    "    \"«зубная щётка»\",\n",
    "    \"«шампунь»\",\n",
    "    \"«мыло»\",\n",
    "    \"«расчёска»\",\n",
    "    \"«ноутбук»\",\n",
    "    \"«зарядка»\",\n",
    "    \"«наушники»\",\n",
    "    \"«паспорт»\",\n",
    "    \"«зонт»\",\n",
    "    \"«книга»\",\n",
    "    \"«вода»\",\n",
    "    \"«кинь футболку»\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:18:59.040007800Z",
     "start_time": "2025-03-20T08:18:59.013006800Z"
    }
   },
   "id": "39349fe352329ad2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_labels = [\n",
    "    [1],\n",
    "    [1], \n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1], \n",
    "    [1], \n",
    "    [1],\n",
    "    [1, 2],\n",
    "    *[[1]] * 10,\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1], \n",
    "    [1, 2],\n",
    "    [0, 1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1], \n",
    "    [1, 0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 1], \n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1, 2],\n",
    "    [0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 2],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 2, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 2, 2], \n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1, 2],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1],\n",
    "    [0, 1, 2],\n",
    "    [0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 2],\n",
    "    [1, 0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 2],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 2],\n",
    "    [1, 2],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 2],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1],\n",
    "    *[[1]] * 7,\n",
    "    [1, 2],\n",
    "    *[[1]] * 10,\n",
    "    [0, 1]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:18:59.875703100Z",
     "start_time": "2025-03-20T08:18:59.836704600Z"
    }
   },
   "id": "106b630ef55b27db"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['футболка', 'трусы', 'носки', 'джинсы', 'куртка', 'шорты', 'кроссовки', 'сандалии', 'зубная щётка', 'шампунь', 'мыло', 'расчёска', 'ноутбук', 'зарядка', 'наушники', 'паспорт', 'зонт', 'книга', 'вода', 'кинь две футболки', 'ноутбук и зарядка', 'зубная щётка', 'возьми трусы', 'носки', 'джинсы', 'куртка', 'не забудь зонт', 'два паспорта', 'сандалии и шорты', 'добавь шампунь', 'забери расчёску', 'кинь пару трусов', 'возьму куртку и шапку', 'не забудь про воду', 'захвати зонт и книгу', 'бери больше футболок', 'хватит одной зарядки', 'положу расчёску и мыло', 'нужно взять два паспорта', 'кинь футболку', 'трусы и носки', 'джинсы и куртка', 'забери зарядку', 'шампунь и мыло', 'расчёска и зубная щётка', 'два паспорта', 'сандалии и шорты', 'вода и еда', 'наушники и книга', 'не забудь зонт', 'футболка и шорты', 'тапки и халат', 'куртка и шапка', 'три футболки', 'ноутбук и мышь', 'расчёска и заколки', 'паспорт и билеты', 'салфетки и спрей', 'солнцезащитные очки', 'плеер и наушники', 'термокружка и ложка', 'зонт и перчатки', 'рюкзак и термос', 'купальник и полотенце', 'пижама и тапочки', 'фотоаппарат и карта памяти', 'аптечка и бинты', 'документы и деньги', 'кофта и свитер', 'ключи и документы', 'плеер и зарядка', 'еда и вода', 'книги и блокнот', 'плеер, наушники, зарядка', 'зубная щётка, паста', 'мыло, шампунь, гель', 'расчёска, заколки, резинки', 'одежда, обувь, аксессуары».', 'кинь футболку', 'трусы и носки', 'джинсы и куртка', 'зарядка для ноутбука', 'шампунь и мыло', 'расчёска и зеркало', 'тапки и халат', 'паспорт и билеты', 'зонт и перчатки', 'вода и еда', 'наушники и книга', 'три футболки', 'две пары носков', 'одни джинсы', 'одна куртка', 'забери зарядку', 'не забудь расчёску', 'добавь тапочки', 'возьми шампунь', 'кинь зубную щётку', 'два паспорта', 'сандалии и шорты', 'солнцезащитные очки', 'фотоаппарат и карта памяти', 'плеер и наушники', 'ключи от дома', 'дождевик и зонт', 'спортивная одежда', 'подушка и одеяло', 'аптечка и пластырь', 'документы и деньги', 'рюкзак и сумка', 'тёплая одежда', 'лёгкая одежда', 'обувь для тренировок', 'кофта и шапка', 'ботинки и носки', 'косметичка и зеркало', 'еда и напитки', 'спальные принадлежности', 'купальник и полотенце', 'инструменты и принадлежности».', 'футболка', 'трусы', 'носки', 'джинсы', 'куртка', 'тапки', 'кроссовки', 'зубная щётка', 'шампунь', 'мыло', 'расчёска', 'ноутбук', 'зарядка', 'наушники', 'паспорт', 'зонт', 'книга', 'вода', 'кинь футболку']\n"
     ]
    }
   ],
   "source": [
    "train_texts_frm = []\n",
    "for txt in train_texts:\n",
    "    txt = txt.strip('«»')\n",
    "    train_texts_frm.append(txt)\n",
    "    \n",
    "print(train_texts_frm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T08:19:01.156147700Z",
     "start_time": "2025-03-20T08:19:01.124146Z"
    }
   },
   "id": "37af131fd54a987d"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['[CLS]', 'Воз', '##ьм', '##у', 'футболку', 'и', 'зуб', '##ную', 'щет', '##ку', '[SEP]']\n",
      "Labels: [[-100, 0, 0, 0, 1, 0, 1, 1, 2, 2, -100], [-100, 0, 0, 0, 0, 1, 1, -100, -100, -100, -100], [-100, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 0, 0, 0, 0, 1, -100, -100, -100, -100, -100]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/3 : < :, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=3, training_loss=0.8292059898376465, metrics={'train_runtime': 17.3531, 'train_samples_per_second': 0.692, 'train_steps_per_second': 0.173, 'total_flos': 67366179144.0, 'train_loss': 0.8292059898376465, 'epoch': 3.0})"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Загрузка предобученной модели и токенизатора\n",
    "model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Подготовка данных (пример)\n",
    "#train_texts = ['Возьму футболку и зубную щетку', 'Хочу и возьму Футболку', 'хочу футболку', 'хочу и возьму футболку']\n",
    "#train_labels = [\n",
    "    #[0, 1, 0, 1, 2], [0, 0, 0, 1], [0, 1], [0, 0, 0, 1]\n",
    "#]\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_and_align_labels(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, padding=True)  # Убрали is_split_into_words=True\n",
    "    aligned_labels = []\n",
    "    \n",
    "    for i, label_seq in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Теперь word_ids будет правильной длины\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Игнорируем CLS, SEP и паддинги\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_seq[word_idx])  # Основная метка\n",
    "            else:\n",
    "                label_ids.append(label_seq[word_idx])  # Повторяем метку для раздробленных токенов\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "\n",
    "    print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][0]))     # это я добавил\n",
    "    \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    \n",
    "    print(\"Labels:\", tokenized_inputs[\"labels\"])            # и это\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Конвертация данных (ИСПРАВЛЕНО)\n",
    "train_encodings = tokenize_and_align_labels(train_texts, train_labels)\n",
    "\n",
    "# Создаем кастомный класс Dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: torch.tensor(val[idx])\n",
    "            for key, val in self.encodings.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Создаем экземпляр Dataset\n",
    "train_dataset = NERDataset(train_encodings)\n",
    "\n",
    "# Обучение\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()  # Исправлено: train() вместо Train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-20T06:25:30.484075Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # Токенизируем входной текст\n",
    "    tokens = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Прогоняем через модель\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Получаем предсказания\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "    \n",
    "    # Декодируем токены обратно в слова\n",
    "    tokens_decoded = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"].squeeze().tolist())\n",
    "    \n",
    "    # Выводим результат\n",
    "    print(\"Токены:\", tokens_decoded)\n",
    "    print(\"Предсказанные метки:\", predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T06:25:56.620302400Z",
     "start_time": "2025-03-20T06:25:56.589726400Z"
    }
   },
   "id": "b86bb092a5108b96"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: ['[CLS]', 'возьм', '##у', 'футболку', '[SEP]']\n",
      "Предсказанные метки: [0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "predict('возьму футболку')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-20T06:26:07.860219400Z",
     "start_time": "2025-03-20T06:26:07.666440100Z"
    }
   },
   "id": "2512cabd22e150"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "947fded50a62fbe9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
